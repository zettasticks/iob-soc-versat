#!/usr/bin/python3

#####
# This tests the entire iob-soc system, whose entry point is the top level makefile.
# As such, we build everything on top of the makefile used by iob-soc to setup the project
# We are essentially testing that the makefile works for all the combination of parameters and tests that we want to test.
#####

#####
# Some important concepts
#  We define two main zones (folders) called testCache and testCacheGood.
#    - testCache contains the files generated by a test run. The "run" command and "run-only" both update the testCache
#    - testCacheGood is a cache that stores the files of the last good run of a test but only if performed by the "run" command. "run-only" does not update this even if the test was good
#
#  TestData always stores information related to the testCache files.
#    - If the testCache was generated from a command that had a bug then the TestData will store that info.
#    - testCacheGood does not need any info because we know that it is always "good". Its purpose is to just be able to 
#      compute the diff between implementations.
#
#  We also define a helpful zone called testCacheLastGood that saves the result from testCacheGood before any overwriting occurs.
#    - For the case where user mistakenly uses the "run" command instead of "run-only"
#
#  Any run command first starts by running versat and checking if the files generate contain the same hash as stored inside the test info file.
#    - If true, then it means that the files inside the testCache are very likely the same files and therefore there is no point in running the full test.
#
#  A Diff command looks at the files inside the testCache and testCacheGood and reports any difference.
#    - The Diff command will perform a "run-only" command by default.
#    - If diff command is run with only one testcase then it outputs the differences on the terminal directly
#    - Otherwise we only see if a diff exists or not.
#
#  The codebase is multithreaded and every test runs individually.
#    - Only simple locking mechanisms are used to make sure that we write test data as soon as possible (so user can abort at any time) while preventing conflicts
#

#### TODO ####
# The diff part is really brittle.
# First, since the pc-emul.txt and sim-run.txt files are only generated when running Versat
#   we run into the problem that the cached stuff messes with the file comparison
#   since the files do not exist.
# Second, the test software file is not copied into the test caches meaning that it is not part 
#   of the comparison. If the change is inside the the software test part then it is
#   not reflected on the diff.
# Overall need more usage to get a feeling for things to implement/change.
##############

#LEFT HERE

# TODO:
# Test code is kinda of a mess after addition of subtests. 
# Also need lock the fact that makefile and this test need to agree on names for things. It might be helpful to put everything that depends on this kinda of stuff in functions that make it easy to change if needed, especially as we add more parameters
#
# Sometimes I make a change and that causes some tests to run that I did not expect to.
#   If they pass, they overwrite the testCacheGood and so I cannot see immediatly what changed. I could just revert the change and rerun the tests to see, but I kinda want another cache to save the trouble.
#     Something like a testCacheGoodPrevious where we saved the previous testCacheGood if it existed. That way I can compare the new run with the old good and see if the change was intended or not.
#
# JSON parsing/encoding/decoding needs to be automatic otherwise becomes clubersome to extend.
#    At the same time, I want proper error reporting if a member is misspelled or something like that, so a little bit rigit is fine if proper error reporting/handling 
#      Code defines what members the json is supposed to have and the encoding/decoding/parsing is done automatic from that
# Add a command that reenables tests, runs them, and disables them if they fail. (Instead of doing a enable + disabled-failed)
# When any test has a change in the accel data, it should report it. If the amount of configs/states bits decreases or increases I should know about it.
# Can easily add per test information. Something like custom timeouts for commands and stuff like that.
# Can also start saving some info regarding the tests itself, like the average amount of time spend per versat and stuff like that.
#    Would really like to start saving the size of things controlable by Versat (amount of buffers added, muxes added, graph weights [when we get to it], etc...)
# 
#
# For things like architecture change at the hardware level, it should be something that is done for all the tests at the same time.
#   Our maybe it is better if we do it by group. Things like AXI_DATA_W changing does not affect config share and stuff like that, so no point in running those tests for those cases. We want to run the VRead/VWrite tests and stuff like that

import subprocess as sp
import threading
from threading import Lock
import sys
import json
import argparse
import codecs
import queue
import time
import traceback
import shutil
import os
import random
import signal
import concurrent.futures
import io
from dataclasses import dataclass
from enum import Enum,auto
from pprint import pprint
from difflib import context_diff


# Globals 
# This are stored inside the testInfo.json and are basically global (read first and set before calling any testcode)
DEFAULT_ARGS = None
SAME_ARGS = None

DIFF_SINGLE = None

SIM = False

# This only affects the TestCacheGood. The normal TestCache is always being deleted and overwritten.
SAVE_ENABLED = False

amountOfThreads = 8

COLOR_BASE   = '\33[0m'
COLOR_GREEN  = '\33[32m'
COLOR_RED    = '\33[31m'
COLOR_YELLOW = '\33[33m'
COLOR_BLUE   = '\33[34m'
COLOR_MAGENTA= '\33[35m'
COLOR_CYAN   = '\33[36m'
COLOR_WHITE  = '\33[37m'

TEST_CACHE_FOLDER_NAME = "../AAA_testCache"
TEST_CACHE_GOOD_FOLDER_NAME = "../AAA_testCacheGood"

def DeleteTestDir(testName):
   path = f"./{TEST_CACHE_FOLDER_NAME}/{testName}"
   shutil.rmtree(path,ignore_errors=True)

def TestDir(testName):
   path = f"./{TEST_CACHE_FOLDER_NAME}/{testName}"
   os.makedirs(path,exist_ok=True)
   return path

def LastGoodTestDir(testName):
   path = f"./{TEST_CACHE_GOOD_FOLDER_NAME}/{testName}"
   os.makedirs(path,exist_ok=True)
   return path

# Order of these are important. Assuming that failing sim-run means that it passed pc-emul 
class Stage(Enum):
   DISABLED = auto()
   DISABLED_FAILING = auto()
   NOT_WORKING = auto()
   SHOULD_FAIL = auto()
   VERSAT = auto()
   # These must be in order. It goes PC_EMUL > SIM_RUN > FPGA_RUN (not implemented and probably never will)
   PC_EMUL = auto()
   SIM_RUN = auto()
   FPGA_RUN = auto()

class ErrorType(Enum):
   NONE = auto()
   EXCEPT = auto()
   TIMEOUT = auto()
   PROGRAM_ERROR = auto()
   TEST_FAILED = auto()

class ErrorSource(Enum):
   NO_SOURCE = auto()
   VERSAT = auto()
   HASHER = auto()
   MAKEFILE = auto()
   PC_EMUL = auto()
   SIM_RUN = auto()

@dataclass
class Error():
   error: ErrorType = ErrorType.NONE
   source: ErrorSource = ErrorSource.NO_SOURCE

   def __repr__(self):
      return "Error[" + self.error.name + ":" + self.source.name + "]"

def IsError(errorType):
   if(type(errorType) == Error):
      return IsError(errorType.error)

   assert(type(errorType) == ErrorType) 
   res = (errorType != ErrorType.NONE)
   return res

class WorkState(Enum):
   INITIAL = auto()
   PROCESS = auto()
   FINISH = auto()

@dataclass
class VersatAcceleratorData:
   configBits: str | None = None
   stateBits: str | None = None
   memUsed: str | None = None
   unitsUsed: str | None = None

def ParseAccelData(jsonInfo):
   # TODO: Find a way of automatizing this if we end up storing more data
   configBits = jsonInfo['configBits'] if 'configBits' in jsonInfo else None
   stateBits = jsonInfo['stateBits'] if 'stateBits' in jsonInfo else None
   memUsed = jsonInfo['memUsed'] if 'memUsed' in jsonInfo else None
   unitsUsed = jsonInfo['unitsUsed'] if 'unitsUsed' in jsonInfo else None

   return VersatAcceleratorData(configBits,stateBits,memUsed,unitsUsed)

@dataclass
class TestInfo:
   parent: 'ParentTestInfo'
   args: str = ""
   tokens: int = 0
   hashVal: int = 0
   stage: Stage = Stage.NOT_WORKING
   error: Error = Error()
   accelData: VersatAcceleratorData | None = None
   cached: bool = False

   def __eq__(self,other):
      # Need to override eq so that we do not enter a loop because of the parent variable

      res = self.args == other.args
      res |= self.tokens == other.tokens
      res |= self.hashVal == other.hashVal
      res |= self.stage == other.stage
      res |= self.accelData == other.accelData

      return res

   # HACKY but works for now
   def __getattr__(self,name):
      if(name == "finalStage"):
         return self.parent.finalStage
      elif(name == "name"):
         return self.NameWithArgsEmbedded()
      elif(name == "expectedError"):
         return self.parent.expectedError
      else:
         self.__getattribute__(name)

   def NameWithArgsEmbedded(self):
      name = self.parent.name
      args = self.args

      if(args and len(args) > 0):
         sanitizedArgs = args.replace("--","_")
         sanitizedArgs = sanitizedArgs.replace("-","_")
         name = name + sanitizedArgs

      return name

   def IsDisabled(self):
      res = (self.parent.finalStage in [Stage.DISABLED,Stage.DISABLED_FAILING])
      return res

   def Finished(self):
      ranEverything = self.stage == self.parent.finalStage

      return ranEverything

   def NoErrors(self):
      noError = (self.error.error == ErrorType.NONE)

      return noError

   def AnyErrors(self):
      anyError = (self.error.error != ErrorType.NONE)

      return anyError

   def Passed(self):
      return self.NoErrors() and self.Finished()

@dataclass
class ParentTestInfo:
   name: str
   finalStage: Stage
   tempDisabledStage: Stage
   comment: str
   subTests: list[TestInfo]
   expectedError: str = ""

   def __hash__(self):
      return hash(self.name)

@dataclass
class TestData:
   defaultArgs: str
   sameArgs: str
   tests: list[TestInfo]

class MyJsonEncoder(json.JSONEncoder):
   def default(self,o):
      if(type(o) == TestData or type(o) == VersatAcceleratorData):
         return vars(o)   
      elif(type(o) == TestInfo):
         asDict = vars(o)
         asDict = {x:y for x,y in asDict.items() if y is not None}
         return asDict
      elif(type(o) == Error):
         return {"error" : o.error.name,"source" : o.source.name}
      elif(type(o) == Stage):
         return o.name
      else:
         return super().default(o)

def ParseJson(jsonInfo,jsonData):
   defaultArgs = jsonInfo['defaultArgs']
   sameArgs = jsonInfo['sameArgs']

   parentTestList = []
   for test in jsonInfo['tests']:

      # Check if all the contents inside a test are valid
      # TODO: There is probably a better way of doing this.
      for member in test:
         if not member in ["name","finalStage","comment","tokens","hashVal","stage","tempDisabledStage","accelData","subTests","expectedError"]:
            print(f"Member '{member}' was not found in the test:")
            print(test)
            sys.exit(0)

      name = test['name']
      finalStage = Stage[test['finalStage']]
      comment = test['comment'] if 'comment' in test else None
      tempDisabledStage = Stage[test['tempDisabledStage']] if 'tempDisabledStage' in test else None
      subTests = test['subTests'] if 'subTests' in test else None
      expectedError = test['expectedError'] if "expectedError" in test else None

      testList = []
      info = ParentTestInfo(name,finalStage,tempDisabledStage,comment,testList,expectedError)
      parentTestList.append(info)

      if(subTests == None):
         test = TestInfo(info)
         trueName = test.NameWithArgsEmbedded()

         data = jsonData[trueName] if jsonData and trueName in jsonData else None
         if(data):
            test.tokens = int(data['tokens']) if 'tokens' in data else None
            test.hashVal = int(data['hashVal']) if 'hashVal' in data else None
            test.stage = Stage[data['stage']] if 'stage' in data else Stage.NOT_WORKING
            test.error = Error(ErrorType[data['error']['error']],ErrorSource[data['error']['source']]) if "error" in data else Error()
            test.accelData = ParseAccelData(data['accelData']) if 'accelData' in data else None

         testList.append(test)
      else :
            for subTest in subTests:
               args = subTest['args'] if "args" in subTest else None
               test = TestInfo(info,args)
               trueName = test.NameWithArgsEmbedded()

               data = jsonData[trueName] if jsonData and trueName in jsonData else None

               if(data):
                  test.tokens = int(data['tokens']) if 'tokens' in data else None
                  test.hashVal = int(data['hashVal']) if 'hashVal' in data else None
                  test.stage = Stage[data['stage']] if 'stage' in data else Stage.NOT_WORKING
                  test.error = Error(ErrorType[data['error']['error']],ErrorSource[data['error']['source']]) if "error" in data else Error()
                  test.accelData = ParseAccelData(data['accelData']) if 'accelData' in data else None

               testList.append(test)

   allTests = []
   for parent in parentTestList:
      for test in parent.subTests:
         allTests.append(test)

   return TestData(defaultArgs,sameArgs,allTests)

# Global data output management. We want to save data as fast as possible
# so that we can still save some progress even if we exit early

printMutex = Lock()
testMutex = Lock()
outputMutex = Lock()
testData = None

def AddTestToTestData(test):
   trueName = test.NameWithArgsEmbedded()

   asDict = {}
   if(test.args):
      asDict["args"] = test.args
   if(test.tokens):
      asDict["tokens"] = test.tokens
   if(test.hashVal):
      asDict["hashVal"] = test.hashVal
   if(test.stage):
      asDict["stage"] = test.stage
   if(test.error):
      asDict["error"] = test.error
   if(test.accelData):
      asDict["accelData"] = test.accelData

   with testMutex:
      testData[trueName] = asDict

def SaveTest(test):
   AddTestToTestData(test)

   with outputMutex:
      if(test.Passed() and SAVE_ENABLED):
         trueName = test.NameWithArgsEmbedded()
         lastGoodLoc = LastGoodTestDir(trueName)
         pathLoc = TestDir(trueName)

         print(f"Saving {pathLoc} to {lastGoodLoc}")

         shutil.rmtree(lastGoodLoc,ignore_errors=True)
         shutil.copytree(pathLoc,lastGoodLoc,dirs_exist_ok=True)            

      with open(jsonTestDataPath,"w") as file:
         json.dump(testData,file,cls=MyJsonEncoder,indent=2)

def signal_handler(sig, frame):
   print("Forced termination\n")

   # Do not want to terminate if we are in the middle of doing any form of IO
   printMutex.acquire()
   testMutex.acquire()
   outputMutex.acquire()
   os._exit(0)

def signal_handler_terminate_immediatly(sig,frame):
   print("Forced termination\n")

   os._exit(0)

# Used to find values in the form "NAME:VAL"
def FindAndParseValue(content,valueToFind):
   for line in content.split("\n"):
      tokens = line.split(":")

      if(tokens[0] == valueToFind):
         return tokens[1].strip()

   return None

def FindAndParseFilepathList(content):
   filePathList = []
   for line in content.split("\n"):
      tokens = line.split(" ")

      fileName = None
      fileType = None

      if(len(tokens) >= 2):
         if(tokens[0] == "Filename:"):
            fileName = tokens[1]
      if(len(tokens) >= 4):
         if(tokens[2] == "Type:"):
            fileType = tokens[3]

      if(fileName):
         filePathList.append(fileName)

   return filePathList

def GenerateProgramOutput(args,subprocessResult):
   decoder = codecs.getdecoder("utf-8")
   output = "" if subprocessResult.stdout == None else decoder(subprocessResult.stdout)[0]
   errorOutput = "" if subprocessResult.stderr == None else decoder(subprocessResult.stderr)[0]

   header = "\n============\n"

   res = args + "\n\n" + header + "   stdout" + header + "\n" + output + header + "   stderr" + header + "\n" + errorOutput

   return res

def RunVersat(testName,testFolder,versatExtra):
   args = ["./submodules/VERSAT/versat"]

   args += ["./versatSpec.txt"]
   args += ["-O",f"{testFolder}/software"]
   args += ["-o",f"{testFolder}/hardware"]
   args += ["-t",f"{testName}"]

   if(versatExtra):
      args += versatExtra.split(" ")

   result = None
   try:
      #print(args)
      #print(*args)
      result = sp.run(args,capture_output=True,timeout=10) # Maybe a bit low for merge based tests, eventually add timeout 'option' to the test itself
   except sp.TimeoutExpired as t:
      return Error(ErrorType.TIMEOUT,ErrorSource.VERSAT),[],None,GenerateProgramOutput(" ".join(args),t)
   except Exception as e:
      print(f"Except on calling Versat:{e}") # This should not happen
      return Error(ErrorType.EXCEPT,ErrorSource.VERSAT),[],None,""

   returnCode = result.returncode

   decoder = codecs.getdecoder("utf-8")
   output = decoder(result.stdout)[0]
   error = decoder(result.stderr)[0]

   #print(returnCode,output,error)

   if(returnCode != 0):
      return Error(ErrorType.PROGRAM_ERROR,ErrorSource.VERSAT),[],None,GenerateProgramOutput(" ".join(args),result)

   filePathList = FindAndParseFilepathList(output)

   data = VersatAcceleratorData()   
   data.configBits = FindAndParseValue(output,"CONFIG_BITS")
   data.stateBits = FindAndParseValue(output,"STATE_BITS")
   data.memUsed = FindAndParseValue(output,"MEM_USED")
   data.unitsUsed = FindAndParseValue(output,"UNITS")

   # Parse result.
   return Error(),filePathList,data,GenerateProgramOutput(" ".join(args),result)

def ComputeFilesTokenSizeAndHash(files):
   args = ["./submodules/VERSAT/tool_build/calculateHash"] + files

   result = None
   try:
      result = sp.run(args,capture_output=True,timeout=5)
   except sp.TimeoutExpired as t:
      return Error(ErrorType.TIMEOUT,ErrorSource.HASHER),-1,-1
   except Exception as e:
      print(f"Except on ComputeHash:{e}")
      return Error(ErrorType.EXCEPT,ErrorSource.HASHER),-1,-1

   returnCode = result.returncode
   decoder = codecs.getdecoder("utf-8")
   output = decoder(result.stdout)[0]
   errorOutput = decoder(result.stderr)[0]

   if(returnCode == 0):
      toFind = "HASH_RESULT:"
      index = output.find(toFind)

      result = output[index + len(toFind):]

      amountOfTokens,hashVal = [int(x) for x in result.split(":")]

      return Error(),amountOfTokens,hashVal
   else:
      print(returnCode,output,errorOutput)
      return Error(ErrorType.PROGRAM_ERROR,ErrorSource.HASHER),-1,-1

def GetTestFolderName(testName,extraArgs,timeout=2):
   result = None
   try:
      command = " ".join(["make","test-folder",f"TEST={testName}",extraArgs if extraArgs else ""])

      result = sp.run(command,capture_output=True,shell=True,timeout=timeout)
   except sp.TimeoutExpired as t:
      return Error(ErrorType.TIMEOUT,ErrorSource.MAKEFILE),GenerateProgramOutput(command,t)
   except Exception as e:
      print(f"Except on calling makefile:{e}")
      return Error(ErrorType.EXCEPT,ErrorSource.MAKEFILE),""

   returnCode = result.returncode

   assert returnCode == 0

   decoder = codecs.getdecoder("utf-8")
   output = "" if result.stdout == None else decoder(result.stdout)[0]

   return output.strip()

# Probably do not want to use makefile, but for now...
def RunMakefile(target,testName,extraArgs,timeout=60):
   result = None
   try:
      command = " ".join(["make",target,f"TEST={testName}",extraArgs if extraArgs else ""])

      result = sp.run(command,capture_output=True,shell=True,timeout=timeout)
   except sp.TimeoutExpired as t:
      return Error(ErrorType.TIMEOUT,ErrorSource.MAKEFILE),GenerateProgramOutput(command,t)
   except Exception as e:
      print(f"Except on calling makefile:{e}")
      return Error(ErrorType.EXCEPT,ErrorSource.MAKEFILE),""

   returnCode = result.returncode

   return Error(),GenerateProgramOutput(command,result)

def CheckTestPassed(testOutput):
   if("TEST_RESULT:TEST_PASSED" in testOutput):
      return True
   elif("TEST_RESULT:TEST_FAILED" in testOutput):
      return False
   else:
      return False

def SaveOutput(testName,fileName,output):
   testTestDir = TestDir(testName)
   with open(testTestDir + f"/{fileName}.txt","w") as file:
      file.write(output)

def PerformTest(test):
   # This function was previously taking the output from the makefile and checking the files using the hasher.
   # This was done because there might be changes from the sim-run and the pc-emul files (stuff like 32bit vs 64 bit addresses and stuff like that)
   # (Although must of the changes right now are "abstracted" inside the verilator makefile, so the hardware is the same (or should be the same))
   # Regardless. If we eventually start making pc-emul and sim-run different, we need to start calculating the hash for each type (pc-emul vs sim-run)
   # Only handle this case when we need it.
   if SIM:
      return test

   testName = test.parent.name
   trueName = test.NameWithArgsEmbedded()

   args = None
   makefileArg = None

   if(test.args):
      makefileArg = ParseVersatArgsIntoMakefile(test.args)

   stage = test.stage
   if stage == Stage.PC_EMUL:
      testFolderName = GetTestFolderName(trueName,makefileArg)

      # TODO: The fast rules appear to not work correctly. 
      #       Would like to have this working but already spent too much time on this
      if(False and os.path.isdir(testFolderName)):
         error,output = RunMakefile("fast-pc-emul",testName,makefileArg)
         SaveOutput(trueName,"fast-pc-emul",output)
      else:
         error,output = RunMakefile("clean pc-emul-run",testName,makefileArg)
         SaveOutput(trueName,"pc-emul",output)

      if(IsError(error)):
         test.error = error
         return test

      testPassed = CheckTestPassed(output)
      if(testPassed):
         test.error = Error()
      else:
         test.error = Error(ErrorType.TEST_FAILED,ErrorSource.PC_EMUL)
      return test
   if stage == Stage.SIM_RUN: 
      # TODO: fast-sim-run appears to be causing problems, but I kinda would like to make this work.
      #       It might be a problem in the makefiles.
      error,output = RunMakefile("sim-run",testName,makefileArg,120)
      SaveOutput(trueName,"sim-run",output)

      if(IsError(error)):
         test.error = error
         return test

      testPassed = CheckTestPassed(output)
      if(testPassed):
         test.error = Error()
      else:
         test.error = Error(ErrorType.TEST_FAILED,ErrorSource.SIM_RUN)

      return test
   if stage == Stage.FPGA_RUN:
      # Not implemented yet, 
      test.error = Error()

   print(f"Error, PerformTest called with: {stage}. Fix this")

def GeneratePad(word,amount,padding = '.'):
   return padding * (amount - len(word))

MAX_NAME_LENGTH = -1
def PrintTestResult(testName,color,condition,partialVal = None,cached = None,comments = None):
   global MAX_NAME_LENGTH
   firstColumnSize = MAX_NAME_LENGTH

   firstPad = ' ' + GeneratePad(testName,firstColumnSize - 1)
   secondPad = ' '
   #print(f"{testName}{firstPad}{secondPad}{color}{condition}{COLOR_BASE}{partialVal}{cached}{comments}")

   finalStr = f"{testName}{firstPad}{secondPad}{color}{condition}{COLOR_BASE}"
   if(partialVal):
      finalStr = finalStr + partialVal
   if(cached):
      finalStr = finalStr + cached
   if(comments):
      finalStr = finalStr + comments
   print(finalStr)

def PrintResult(test):
   name = test.NameWithArgsEmbedded()

   finalStage = test.parent.finalStage
   stage = test.stage

   testName = name
   actualComment = test.parent.comment
   comments = f" - {actualComment}" if actualComment else ""
   failing = "FAIL"
   partial = f"PARTIAL"
   partialVal = ""
   ok = "OK"
   cached = "(cached)" if test.cached else ""

   if(stage == Stage.PC_EMUL):
      ok = "OK[1]"
   if(stage == Stage.SIM_RUN):
      ok = "OK[2]"

   testPassed = test.NoErrors()
   testRanEverything = test.Finished()

   condition = None
   color = None

   if(not testRanEverything):
      partialVal = f"[{stage.name}/{finalStage.name}]"

   if(test.IsDisabled()):
      condition = "DISABLED" if finalStage == Stage.DISABLED else "DISABLED_FAILING"
      partialVal = ""
      color = COLOR_CYAN if finalStage == Stage.DISABLED else COLOR_YELLOW
   elif(stage == Stage.NOT_WORKING):
      condition = failing + " " + stage.name + f"[{finalStage.name}]"
      partialVal = ""
      color = COLOR_RED
   elif(IsError(test.error)):
      color = COLOR_RED
      condition = f"{test.error.error.name}:{test.error.source.name}"
   elif(cached):
      condition = partial if not testRanEverything else ok
      color = COLOR_GREEN if testRanEverything else COLOR_YELLOW       
   elif(testPassed):
      condition = ok
      color = COLOR_GREEN
   else:
      print(f"Stage not handled: {stage},{finalStage}")

   PrintTestResult(testName,color,condition,partialVal,cached,comments)

def CppLocation(testName):
   return f"./software/src/Tests/{testName}.cpp"

def ThreadMain(workQueue,resultQueue,id):
   while(True):
      test = workQueue.get()
      
      try:
         test = GetCurrentTestState(test)
      except Exception as e:
         print(f"Exception reached ThreadMain:")
         traceback.print_exception(e)

         test.error = Error(ErrorType.EXCEPT,ErrorSource.NO_SOURCE)
      
      resultQueue.put(test)
      workQueue.task_done()

def CalculateMaxLengthOfTestNames(testList):
   maxNameLength = 0
   for test in testList:
      trueName = test.NameWithArgsEmbedded()
      maxNameLength = max(maxNameLength,len(trueName))
   return (maxNameLength + 2) # +2 to have some space between name and result

# TODO: Probably not gonna need to improve this any time soon, but if we do need to add more stuff, rewrite this.
def ParseVersatArgsIntoMakefile(versatArgs):
   versatArgs = versatArgs.strip()
   if(versatArgs == None or versatArgs == ''):
      return ""

   if(versatArgs == '--profile'):
      return "DO_PROFILE=T"
   if(versatArgs == '-b32'):
      return "AXI_DATA_W=32"
   elif(versatArgs == '-b64'):
      return "AXI_DATA_W=64"
   elif(versatArgs == '-b128'):
      return "AXI_DATA_W=128"
   elif(versatArgs == '-b256'):
      return "AXI_DATA_W=256"
   else:
      print(f"[ParseVersatArgsIntoMakefile] Need to add logic for Versat arg: {versatArgs}")

def TempDisableTest(test):
   test.tempDisabledStage = test.finalStage
   test.finalStage = Stage.DISABLED_FAILING

def ReprintButOrganized(testList):
   def AllCaps(name):
      for x in name:
         if('A' <= x <= "Z"):
            continue
         return False
      return True

   def TestGroups(testName):
      group = []
      splitted = testName.split("_")

      for split in splitted:
         if(AllCaps(split) and len(split) > 3): # LEN > 3 is mainly because of SHA and F stage and the likes.
            group.append(split)

      return group

   testGroups = {}
   for test in testList:
      name = test.NameWithArgsEmbedded()

      groups = TestGroups(name)

      for group in groups:
         testGroups[group] = True

   print("\n\nReprinting results grouped:\n\n")

   for group in testGroups.keys():
      print(f"{COLOR_CYAN}{group}{COLOR_BASE}:")
      for test in testList:
         name = test.NameWithArgsEmbedded()

         groups = TestGroups(name)

         if group in groups:
            PrintResult(test)

      print("")

def ThreadedPrintResult(result):
   with printMutex:
      PrintResult(result)

def GetTestArgs(test):
   if(test.args):
      args = SAME_ARGS + ' ' + test.args
   else:
      args = SAME_ARGS + ' ' + DEFAULT_ARGS

   return args

printTestResultMutex = Lock()
def MutexPrintTestResult(name,color,msg):
   with printTestResultMutex:
      PrintTestResult(name,color,msg)

def ComputeDiff(test):
   try:
      test = GetCurrentTestState(test)
      name = test.NameWithArgsEmbedded()

      if not os.path.exists(TEST_CACHE_GOOD_FOLDER_NAME):
         MutexPrintTestResult(name,COLOR_BLUE,"NO_DATA")
         return

      fileMatch = []
      for root, subdirs, files in os.walk(TEST_CACHE_GOOD_FOLDER_NAME):
         if(not "hardware" in root and not "software" in root):
            continue

         for file in files:
            testCacheGood = os.path.join(root,file)

            fullPath = testCacheGood.replace(TEST_CACHE_GOOD_FOLDER_NAME,TEST_CACHE_FOLDER_NAME)

            fileMatch.append((fullPath,testCacheGood))

      results = []
      differenceAmount = 0
      for now,good in fileMatch:
         contentNow = None
         contentGood = None
         error = False
         try:
            with open(now,"r") as f:
               contentNow = f.readlines()
         except:
            results.append((now,good,[f"File {now} does not exist"]))   
            differenceAmount += 1
            error = True

         try:
           with open(good,"r") as f:
               contentGood = f.readlines()
         except:
            results.append((now,good,[f"File {good} does not exist"]))   
            differenceAmount += 1
            error = True

         if not error:
            res = context_diff(contentNow,contentGood,fromfile=now,tofile=good)

            allDifferences = [x for x in res]
            results.append((now,good,allDifferences))
            differenceAmount += len(allDifferences)

      if(differenceAmount != 0):
         MutexPrintTestResult(name,COLOR_YELLOW,"DIFF")

         if(DIFF_SINGLE):
            with printTestResultMutex:
               for test,good,differences in results:
                  for x in differences:
                     print(x)
      else:
         if(test.AnyErrors()):
            MutexPrintTestResult(name,COLOR_YELLOW,"EQUAL[WITH_ERRORS]")
         else:
            MutexPrintTestResult(name,COLOR_GREEN,"EQUAL")

   except Exception as e:
      traceback.print_exception(e)

def GetCurrentTestState(test):
   name = test.NameWithArgsEmbedded()
   args = GetTestArgs(test)

   finalStage = test.finalStage

   # NOTE: We always delete the testCache just to prevent old files from stickying around.
   DeleteTestDir(name)
   dirPath = TestDir(name)

   versatError,filepaths,versatData,output = RunVersat(test.parent.name,dirPath,args)

   if(finalStage == Stage.SHOULD_FAIL):
      errorsMatch = True
      for errorType in test.expectedError.split(";"):
         errorType = errorType.strip()

         if(not errorType in output):
            errorsMatch = False

      if(errorsMatch):
         test.stage = Stage.SHOULD_FAIL
      return test

   SaveOutput(name,"versat",output)

   if(IsError(versatError)):
      test.error = versatError
      return test

   sourceLocation = CppLocation(test.parent.name)
   filepathsToHash = filepaths + [sourceLocation]
   hashError,tokenAmount,hashVal = ComputeFilesTokenSizeAndHash(filepathsToHash)
   if(IsError(hashError)):
      test.error = hashError
      return test

   testTokens = test.tokens if test.tokens else 0
   testHashVal = test.hashVal if test.hashVal else 0

   if(tokenAmount == testTokens and hashVal == testHashVal):
      test.cached = True
      return test

   test.tokens = tokenAmount
   test.hashVal = hashVal
   test.accelData = versatData
   test.stage = Stage.VERSAT

   stageToProcess = Stage(test.stage.value + 1)
   finalStage = test.finalStage
   while stageToProcess.value < finalStage.value + 1:
      test.stage = stageToProcess
      test = PerformTest(test)

      SaveTest(test)
      ThreadedPrintResult(test)

      if(test.AnyErrors()):
         break

      stageToProcess = Stage(stageToProcess.value + 1)

   return test

def RunTests(testList):
   global amountOfThreads
   amountOfTests = len(testList)

   workQueue = queue.Queue()
   resultQueue = queue.Queue()
   threadList = [threading.Thread(target=ThreadMain,args=[workQueue,resultQueue,x],daemon=True) for x in range(amountOfThreads)]
   for thread in threadList:
      thread.start()

   amountOfWork = 0   

   for test in testList:
      if(test.IsDisabled()):
         PrintResult(test)
         continue

      amountOfWork += 1
      workQueue.put(test)

   resultList = []

   while(amountOfWork > 0):
      try:
         result = resultQueue.get(True,1)
      except queue.Empty as e:
         time.sleep(1)
         continue

      resultList.append(result)
      amountOfWork -= 1

   return resultList

if __name__ == "__main__":
   testInfoJson = None
   jsonTestInfoPath = "testInfo.json"
   jsonTestDataPath = "testData.json"

   # Recompile versat first, otherwise we could have multiple jobs compiling the same exe for no reason
   try:
      args = ["make fast-compile-versat"]
      sp.run(args,capture_output=False,shell=True,timeout=10)
   except sp.TimeoutExpired as t:
      print("Timeout on versat compilation")
      sys.exit(0)
   except Exception as e:
      print("Error on versat compilation")
      print(e)
      sys.exit(0)

   parser = argparse.ArgumentParser(prog="Tester",description="Test Versat, using cache to prevent rerunning unnecessary tests")
   allCommands = ["run","run-only","diff","reset","enable","list"]

   parser.add_argument("command",choices=allCommands)
   parser.add_argument("testFilter",nargs='*')

   arguments = parser.parse_args()

   command = arguments.command
   testFilter = arguments.testFilter if arguments.testFilter else [""]

   try:
      with open(jsonTestInfoPath,"r") as file:
         testInfoJson = json.load(file)
   except Exception as e:
      print(f"Failed to open/parse testInfo file: {e}")
      sys.exit(0)

   try:
      with open(jsonTestDataPath,"r") as file:
         testDataJson = json.load(file)
   except OSError as e:
      testDataJson = None
   except Exception as e:
      print(f"Failed to open/parse testData file: {e}")
      sys.exit(0)

   testInfo = ParseJson(testInfoJson,testDataJson)

   DEFAULT_ARGS = testInfo.defaultArgs
   SAME_ARGS = testInfo.sameArgs

   # Initialize test data.
   # This ensures us that we never lose previous data.
   testData = {}
   for test in testInfo.tests:
      AddTestToTestData(test)

   def Filter(name,filters):
      for fil in filters:
         if(fil in name):
            return True
      return False

   allTests = [test for test in testInfo.tests if Filter(test.NameWithArgsEmbedded(),testFilter)]

   testCount = {}
   for test in testInfo.tests:
      testCount[test.NameWithArgsEmbedded()] = testCount.get(test.NameWithArgsEmbedded(),0) + 1

   for name,count in testCount.items():
      if count > 1:
         print(f"\n\nTest {name} is repeated on the testData.json, remove it\n")
         sys.exit(0)

   if(len(allTests) == 0):
      print("No tests found")
      sys.exit(0)

   MAX_NAME_LENGTH = CalculateMaxLengthOfTestNames(allTests)

   print(f"\n\nFound and processing {len(allTests)} test(s)\n")
   print("\n\n")

   allTestNames = [x.NameWithArgsEmbedded() for x in allTests]

   nameCount : dict[str,int] = {}
   for name in allTestNames:
      nameCount[name] = nameCount.get(name,0) + 1

   repeatedElements = [x for x,y in nameCount.items() if y != 1]
   if(len(repeatedElements)):
      print("There are repeated tests:\n\t","\t\n".join(repeatedElements),sep='')
      print("Exiting. Fix test info and run again.")
      sys.exit(0)

   if(command == "reset"):
      for test in allTests:
         test.tokens = None
         test.hashVal = None
         test.stage = None
         test.accelData = None

   elif(command == "diff"):
      signal.signal(signal.SIGINT, signal_handler_terminate_immediatly)

      DIFF_SINGLE = (len(allTests) == 1)

      with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
         executor.map(ComputeDiff, allTests)

      sys.exit(0)

   elif(command == "run" or command == "run-only" or command == "disable-failed"):
      if(command == "run" or command == "run-only"):
         signal.signal(signal.SIGINT, signal_handler)

      if(command == "run"):
         SAVE_ENABLED = True

      resultList : list[TestInfo] = RunTests(allTests)

      ReprintButOrganized(resultList)

      # Run test already updates testData.json
      sys.exit(0)

   elif(command == "enable"):
      for test in allTests:
         if(test.finalStage == Stage.DISABLED_FAILING):
            test.finalStage = test.tempDisabledStage
            test.tempDisabledStage = None
         elif(test.finalStage == Stage.DISABLED):
            print(f"Test {test.name} was not temp disabled, so cannot enable again")

   elif(command == "list"):
      testList = list(sorted(testList,key=lambda x : x.name))
      for test in testList:
         print(test.name)

   print("\n\n\n") # A few new lines to make easier to see results

   for test in testInfo.tests:
      SaveTest(test)

   with open(jsonTestDataPath,"w") as file:
      json.dump(testData,file,cls=MyJsonEncoder,indent=2)

   sys.exit(0)
